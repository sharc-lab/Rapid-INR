{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "a486707e-b8d4-4d4b-95c4-16e416382296",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from torch._C import dtype\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import Any, BinaryIO, List, Optional, Tuple, Union\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from types import FunctionType\n",
    "\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "66c0f55c-daa4-4d2d-a98c-5530f5c31313",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = 28\n",
    "num_layers = 5\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "w0 = 30.0\n",
    "w0_initial = 30.0\n",
    "learning_rate = 5e-4\n",
    "num_iters = 5000\n",
    "device = \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "8ad6f107-42d9-4c8d-bccb-a1facb050b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    \"\"\"Sine activation with scaling.\n",
    "    Args:\n",
    "        w0 (float): Omega_0 parameter from SIREN paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class SirenLayer(nn.Module):\n",
    "    \"\"\"Implements a single SIREN layer.\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_out (int): Dimension of output.\n",
    "        w0 (float):\n",
    "        c (float): c value from SIREN paper used for weight initialization.\n",
    "        is_first (bool): Whether this is first layer of model.\n",
    "        use_bias (bool):\n",
    "        activation (torch.nn.Module): Activation function. If None, defaults to\n",
    "            Sine activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, w0=30., c=6., is_first=False,\n",
    "                 use_bias=True, activation=None):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=use_bias)\n",
    "\n",
    "        # Initialize layers following SIREN paper\n",
    "        w_std = (1 / dim_in) if self.is_first else (sqrt(c / dim_in) / w0)\n",
    "        nn.init.uniform_(self.linear.weight, -w_std, w_std)\n",
    "        if use_bias:\n",
    "            nn.init.uniform_(self.linear.bias, -w_std, w_std)\n",
    "\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    \"\"\"SIREN model.\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool):\n",
    "        final_activation (torch.nn.Module): Activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out, num_layers, w0=30.,\n",
    "                 w0_initial=30., use_bias=True, final_activation=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for ind in range(num_layers):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            layers.append(SirenLayer(\n",
    "                dim_in=layer_dim_in,\n",
    "                dim_out=dim_hidden,\n",
    "                w0=layer_w0,\n",
    "                use_bias=use_bias,\n",
    "                is_first=is_first\n",
    "            ))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        final_activation = nn.Identity() if final_activation is None else final_activation\n",
    "        self.last_layer = SirenLayer(dim_in=dim_hidden, dim_out=dim_out, w0=w0,\n",
    "                                use_bias=use_bias, activation=final_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "2034773b-038c-4b9d-9293-cae0dd7b59b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_coordinates_and_features(img):\n",
    "    \"\"\"Converts an image to a set of coordinates and features.\n",
    "    Args:\n",
    "        img (torch.Tensor): Shape (channels, height, width).\n",
    "    \"\"\"\n",
    "    # Coordinates are indices of all non zero locations of a tensor of ones of\n",
    "    # same shape as spatial dimensions of image\n",
    "    coordinates = torch.ones(img.shape[1:]).nonzero(as_tuple=False).float()\n",
    "    #coordinates = torch.ones(img.shape[1:]).float()\n",
    "    # Normalize coordinates to lie in [-.5, .5]\n",
    "    coordinates = coordinates / (img.shape[1] - 1) - 0.5\n",
    "    # Convert to range [-1, 1]\n",
    "    coordinates *= 2\n",
    "    # Convert image to a tensor of features of shape (num_points, channels)\n",
    "    features = img.reshape(img.shape[0], -1).T\n",
    "    return coordinates, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "dd823def-9fc5-43d3-9512-69f24e253b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def psnr(img1, img2):\n",
    "    \"\"\"Calculates PSNR between two images.\n",
    "    Args:\n",
    "        img1 (torch.Tensor):\n",
    "        img2 (torch.Tensor):\n",
    "    \"\"\"\n",
    "    return 20. * np.log10(1.) - 10. * (img1 - img2).pow(2).mean().log10().to('cpu').item()\n",
    "\n",
    "def get_clamped_psnr(img_recon, img):\n",
    "    \"\"\"Get PSNR between true image and reconstructed image. As reconstructed\n",
    "    image comes from output of neural net, ensure that values like in [0, 1] and\n",
    "    are unsigned ints.\n",
    "    Args:\n",
    "        img (torch.Tensor): Ground truth image.\n",
    "        img_recon (torch.Tensor): Image reconstructed by model.\n",
    "    \"\"\"\n",
    "    return psnr(clamp_image(img_recon), img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "6f4b7836-4d6f-4ddc-9e5c-34a386d91131",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clamp_image(img):\n",
    "    \"\"\"Clamp image values to like in [0, 1] and convert to unsigned int.\n",
    "    Args:\n",
    "        img (torch.Tensor):\n",
    "    \"\"\"\n",
    "    # Values may lie outside [0, 1], so clamp input\n",
    "    img_ = torch.clamp(img, 0., 1.)\n",
    "    # Pixel values lie in {0, ..., 255}, so round float tensor\n",
    "    return torch.round(img_ * 255) / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "b327802b-f760-4846-88ef-e4c0d04e00a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image_file(filename): # Compares 'filename' extension to common image file types.\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "def load_image_path(imgDir):\n",
    "\n",
    "    all_training_files=os.walk(imgDir)\n",
    "    #print(all_training_files)\n",
    "    train_files=[]\n",
    "    train_imageNames=[]\n",
    "    train_nSamples=0\n",
    "    for path,direction,filelist in all_training_files:\n",
    "        files = [file for file in filelist if os.path.isfile(os.path.join(path, file))]\n",
    "        #print(files)\n",
    "        imageNames = [file.split('.')[0] for file in files if is_image_file(file)]\n",
    "        files = [os.path.join(path, file) for file in files if is_image_file(file)]\n",
    "        train_files.append(files)\n",
    "        train_imageNames.append(imageNames)\n",
    "        train_nSamples=train_nSamples+len(files)\n",
    "    train_files=sum(train_files,[])\n",
    "    train_imageNames=sum(train_imageNames,[])\n",
    "    #print(train_imageNames[0])\n",
    "    #print(train_files[0])\n",
    "    print(train_nSamples)\n",
    "    \n",
    "    return train_files, train_imageNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f4cf45-7e80-4956-851d-036847e561b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "3c2eac79-b5f8-4566-b3d6-48e48c2a507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "#imgDir = '/export/hdd/scratch/dataset/mini_imagenet/'\n",
    "imgDir_ref = '/export/hdd/scratch/dataset/mini_imagenet_raw/n01614925'\n",
    "decode_image_path_ref, train_image_names_ref = load_image_path(imgDir_ref)\n",
    "transform_size = transforms.Resize([224,224])\n",
    "psnr_value = np.zeros([10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "a1fb6305-ad8b-4776-999e-b54acd2dd741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n",
      "torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for i in range (10):\n",
    "    img = cv2.imread(decode_image_path_ref[i])\n",
    "    \n",
    "    dim = (224, 224)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    #img = cv2.imread(\"./cifar_10_images/train_cifar10/%d.jpg\"%(i))\n",
    "    #img = np.transpose(img, (2, 0, 1))\n",
    "    #image_reference[i] = img\n",
    "    #img = Image.open(decode_image_path_ref[4]).convert('RGB')\n",
    "    \n",
    "    #img = img.resize((224,224))\n",
    "    #img = img.resize((224,224),Image.ANTIALIAS)\n",
    "    #print(img)\n",
    "    #img_tensor_tran = transforms_test_2(img)\n",
    "    #image_reference = np.asarray(img, dtype = np.float32)\n",
    "    image_tensor_transformed = torch.from_numpy(img)\n",
    "    #image_tensor_transformed = image_tensor.permute(2,0,1)\n",
    "    #image_tensor_transformed = transform_size(image_tensor_1)\n",
    "    print(image_tensor_transformed.shape)\n",
    "    #psnr_value[i] = psnr(img_transformed/255.0, image_tensor_1_transformed/255.0)\n",
    "    \n",
    "    batch_size = 1\n",
    "    img = torch.zeros([ 3,224,224])\n",
    "    coordinates = torch.zeros([img.shape[1] * img.shape[2], 2])\n",
    "    for j in range (batch_size):\n",
    "        coordinates, _= to_coordinates_and_features(img)\n",
    "    \n",
    "    func_rep = Siren(\n",
    "            dim_in=2,\n",
    "            dim_hidden=layer_size,\n",
    "            dim_out=3,\n",
    "            num_layers=num_layers,\n",
    "            final_activation=torch.nn.Identity(),\n",
    "            w0_initial=w0_initial,\n",
    "            w0=w0\n",
    "        ).to(device)\n",
    "    func_rep = func_rep.half()\n",
    "    #func_rep.eval()\n",
    "    #PATH = '/usr/scratch/hchen799/INR/weights_ImageNet/trial_13x49_30fq_5000ep_10im/best_model_4.pt'\n",
    "    func_rep.load_state_dict(torch.load('/usr/scratch/hchen799/INR/weights_ImageNet/trial_5x28_30fq_5000ep_10im/best_model_%d.pt'%(i)))\n",
    "    coordinates = coordinates.to(device)\n",
    "    coordinates = coordinates.half()\n",
    "    output = func_rep(coordinates)\n",
    "    #output = output[:,[2,1,0]]\n",
    "    output = output.reshape(224,224,3)\n",
    "    output = clamp_image(output)\n",
    "    output = output.permute(2,0,1)\n",
    "    output = output.cpu()\n",
    "    #print(output * 255)\n",
    "    #print(\"the raw image is \",image_tensor_transformed)\n",
    "    psnr_value[i] = psnr(output, image_tensor_transformed/255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "3535101a-a01e-46c9-b81b-38a4bea69e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.31619930267334\n"
     ]
    }
   ],
   "source": [
    "psnr_total = 0\n",
    "for i in range (10):\n",
    "    psnr_total = psnr_total + psnr_value[i]\n",
    "psnr_average = psnr_total/10\n",
    "print(psnr_average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ded2ef6-55d3-4575-9d2c-b37163bb1c70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0e542e28-c126-4394-a23b-adfdb24ca015",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img = torch.zeros([ 3,224,224])\n",
    "coordinates = torch.zeros([img.shape[1] * img.shape[2], 2])\n",
    "for i in range (batch_size):\n",
    "    coordinates, _= to_coordinates_and_features(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1798e092-081c-4650-b419-ddd4c20f1ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50176, 2])\n"
     ]
    }
   ],
   "source": [
    "print(coordinates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36364147-783b-45b9-bab7-3974b7e18b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_rep.load_state_dict(torch.load(PATH))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
