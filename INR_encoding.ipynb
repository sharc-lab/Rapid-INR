{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0b40b83c-1ba5-46ed-b4f9-c4b2b9c46446",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from torch import nn\n",
    "from math import sqrt\n",
    "\n",
    "from typing import Dict\n",
    "\n",
    "from torch._C import dtype\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "import tqdm\n",
    "\n",
    "from typing import Any, BinaryIO, List, Optional, Tuple, Union\n",
    "\n",
    "import pathlib\n",
    "\n",
    "from types import FunctionType\n",
    "\n",
    "from PIL import Image, ImageColor, ImageDraw, ImageFont\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb2f62ef-248c-43d4-9f7e-0a2be1fde6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = 28\n",
    "num_layers = 10\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "w0 = 30.0\n",
    "w0_initial = 30.0\n",
    "learning_rate = 5e-4\n",
    "num_iters = 10000\n",
    "device = \"cuda:0\"\n",
    "train_set = '/export/hdd/scratch/dataset/imagenet/train/n01440764' ##\n",
    "\n",
    "logdir = './weights_ImageNet'\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "# ** Insert line to create /resizeimg_in if it doesn't exist\n",
    "log_dir_resize = './resizeimg_in'\n",
    "if not os.path.exists(log_dir_resize):\n",
    "    os.makedirs(log_dir_resize)\n",
    "log_dir_image = './reconimages_ImageNet/trial'\n",
    "if not os.path.exists(log_dir_image):\n",
    "    os.makedirs(log_dir_image)\n",
    "logdir_array = './ImageNet_array'\n",
    "if not os.path.exists(logdir_array):\n",
    "    os.makedirs(logdir_array)\n",
    "logdir_logfiles = './ImageNet_logfiles'\n",
    "if not os.path.exists(logdir_logfiles):\n",
    "    os.makedirs(logdir_logfiles)\n",
    "\n",
    "\n",
    "#train_image_count = 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0361b631-00de-45b4-ba65-21839f24fcde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_image_file(filename): # Compares 'filename' extension to common image file types.\n",
    "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.PNG', '.JPG', '.JPEG'])\n",
    "def load_image_path(imgDir):\n",
    "\n",
    "    all_training_files=os.walk(imgDir)\n",
    "    train_files=[]\n",
    "    train_imageNames=[]\n",
    "    train_nSamples=0\n",
    "    for path,direction,filelist in all_training_files:\n",
    "        files = [file for file in filelist if os.path.isfile(os.path.join(path, file))]\n",
    "        imageNames = [file.split('.')[0] for file in files if is_image_file(file)]\n",
    "        files = [os.path.join(path, file) for file in files if is_image_file(file)]\n",
    "        train_files.append(files)\n",
    "        train_imageNames.append(imageNames)\n",
    "        train_nSamples=train_nSamples+len(files)\n",
    "    train_files=sum(train_files,[])\n",
    "    train_imageNames=sum(train_imageNames,[])\n",
    "    print(train_imageNames[0])\n",
    "    print(train_files[0])\n",
    "    \n",
    "    return train_files, train_imageNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a0173f6c-0cc6-4fac-8482-037efc9e30c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTYPE_BIT_SIZE: Dict[dtype, int] = {\n",
    "    torch.float32: 32,\n",
    "    torch.float: 32,\n",
    "    torch.float64: 64,\n",
    "    torch.double: 64,\n",
    "    torch.float16: 16,\n",
    "    torch.half: 16,\n",
    "    torch.bfloat16: 16,\n",
    "    torch.complex32: 32,\n",
    "    torch.complex64: 64,\n",
    "    torch.complex128: 128,\n",
    "    torch.cdouble: 128,\n",
    "    torch.uint8: 8,\n",
    "    torch.int8: 8,\n",
    "    torch.int16: 16,\n",
    "    torch.short: 16,\n",
    "    torch.int32: 32,\n",
    "    torch.int: 32,\n",
    "    torch.int64: 64,\n",
    "    torch.long: 64,\n",
    "    torch.bool: 1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0ee4057b-a4cf-480e-87c4-c7ba6c0fb503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image_new(image, address):\n",
    "    image = image * 255\n",
    "    image = image.round()\n",
    "    image = torch.clamp(image,0,255)\n",
    "    image_array = image.cpu().numpy()\n",
    "    image_array = image_array.transpose(1,2,0)\n",
    "    cv2.imwrite(address, image_array) \n",
    "\n",
    "\n",
    "def to_coordinates_and_features(img):\n",
    "    \"\"\"Converts an image to a set of coordinates and features.\n",
    "    Args:\n",
    "        img (torch.Tensor): Shape (channels, height, width).\n",
    "    \"\"\"\n",
    "    # Coordinates are indices of all non zero locations of a tensor of ones of\n",
    "    # same shape as spatial dimensions of image\n",
    "    coordinates = torch.ones(img.shape[1:]).nonzero(as_tuple=False).float()\n",
    "    # Normalize coordinates to lie in [-.5, .5]\n",
    "    coordinates = coordinates / (img.shape[1] - 1) - 0.5\n",
    "    # Convert to range [-1, 1]\n",
    "    coordinates *= 2\n",
    "    # Convert image to a tensor of features of shape (num_points, channels)\n",
    "    features = img.reshape(img.shape[0], -1).T\n",
    "    return coordinates, features\n",
    "\n",
    "def model_size_in_bits(model):\n",
    "    \"\"\"Calculate total number of bits to store `model` parameters and buffers.\"\"\"\n",
    "    return sum(sum(t.nelement() * DTYPE_BIT_SIZE[t.dtype] for t in tensors)\n",
    "               for tensors in (model.parameters(), model.buffers()))\n",
    "\n",
    "def bpp(image, model):\n",
    "    \"\"\"Computes size in bits per pixel of model.\n",
    "    Args:\n",
    "        image (torch.Tensor): Image to be fitted by model.\n",
    "        model (torch.nn.Module): Model used to fit image.\n",
    "    \"\"\"\n",
    "    num_pixels = np.prod(image.shape) / 3  # Dividing by 3 because of RGB channels\n",
    "    return model_size_in_bits(model=model) / num_pixels\n",
    "\n",
    "def psnr(img1, img2):\n",
    "    \"\"\"Calculates PSNR between two images.\n",
    "    Args:\n",
    "        img1 (torch.Tensor):\n",
    "        img2 (torch.Tensor):\n",
    "    \"\"\"\n",
    "    return 20. * np.log10(1.) - 10. * (img1 - img2).detach().pow(2).mean().log10().to('cpu').item()\n",
    "\n",
    "def clamp_image(img):\n",
    "    \"\"\"Clamp image values to like in [0, 1] and convert to unsigned int.\n",
    "    Args:\n",
    "        img (torch.Tensor):\n",
    "    \"\"\"\n",
    "    # Values may lie outside [0, 1], so clamp input\n",
    "    img_ = torch.clamp(img, 0., 1.)\n",
    "    # Pixel values lie in {0, ..., 255}, so round float tensor\n",
    "    return torch.round(img_ * 255) / 255.\n",
    "\n",
    "def get_clamped_psnr(img_recon, img):\n",
    "    \"\"\"Get PSNR between true image and reconstructed image. As reconstructed\n",
    "    image comes from output of neural net, ensure that values like in [0, 1] and\n",
    "    are unsigned ints.\n",
    "    Args:\n",
    "        img (torch.Tensor): Ground truth image.\n",
    "        img_recon (torch.Tensor): Image reconstructed by model.\n",
    "    \"\"\"\n",
    "    return psnr(clamp_image(img_recon), img)\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_image(\n",
    "    tensor: Union[torch.Tensor, List[torch.Tensor]],\n",
    "    fp: Union[str, pathlib.Path, BinaryIO],\n",
    "    format: Optional[str] = None,\n",
    "    **kwargs,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save a given Tensor into an image file.\n",
    "    Args:\n",
    "        tensor (Tensor or list): Image to be saved. If given a mini-batch tensor,\n",
    "            saves the tensor as a grid of images by calling ``make_grid``.\n",
    "        fp (string or file object): A filename or a file object\n",
    "        format(Optional):  If omitted, the format to use is determined from the filename extension.\n",
    "            If a file object was used instead of a filename, this parameter should always be used.\n",
    "        **kwargs: Other arguments are documented in ``make_grid``.\n",
    "    \"\"\"\n",
    "\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(save_image)\n",
    "    grid = make_grid(tensor, **kwargs)\n",
    "    # Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\n",
    "    ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\"cpu\", torch.uint8).numpy()\n",
    "    im = Image.fromarray(ndarr)\n",
    "    im.save(fp, format=format)\n",
    "    \n",
    "\n",
    "    \n",
    "def _log_api_usage_once(obj: Any) -> None:\n",
    "\n",
    "    \"\"\"\n",
    "    Logs API usage(module and name) within an organization.\n",
    "    In a large ecosystem, it's often useful to track the PyTorch and\n",
    "    TorchVision APIs usage. This API provides the similar functionality to the\n",
    "    logging module in the Python stdlib. It can be used for debugging purpose\n",
    "    to log which methods are used and by default it is inactive, unless the user\n",
    "    manually subscribes a logger via the `SetAPIUsageLogger method <https://github.com/pytorch/pytorch/blob/eb3b9fe719b21fae13c7a7cf3253f970290a573e/c10/util/Logging.cpp#L114>`_.\n",
    "    Please note it is triggered only once for the same API call within a process.\n",
    "    It does not collect any data from open-source users since it is no-op by default.\n",
    "    For more information, please refer to\n",
    "    * PyTorch note: https://pytorch.org/docs/stable/notes/large_scale_deployments.html#api-usage-logging;\n",
    "    * Logging policy: https://github.com/pytorch/vision/issues/5052;\n",
    "    Args:\n",
    "        obj (class instance or method): an object to extract info from.\n",
    "    \"\"\"\n",
    "    module = obj.__module__\n",
    "    if not module.startswith(\"torchvision\"):\n",
    "        module = f\"torchvision.internal.{module}\"\n",
    "    name = obj.__class__.__name__\n",
    "    if isinstance(obj, FunctionType):\n",
    "        name = obj.__name__\n",
    "    torch._C._log_api_usage_once(f\"{module}.{name}\")\n",
    "    \n",
    "    \n",
    "def make_grid(\n",
    "    tensor: Union[torch.Tensor, List[torch.Tensor]],\n",
    "    nrow: int = 8,\n",
    "    padding: int = 2,\n",
    "    normalize: bool = False,\n",
    "    value_range: Optional[Tuple[int, int]] = None,\n",
    "    scale_each: bool = False,\n",
    "    pad_value: float = 0.0,\n",
    "    **kwargs,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Make a grid of images.\n",
    "    Args:\n",
    "        tensor (Tensor or list): 4D mini-batch Tensor of shape (B x C x H x W)\n",
    "            or a list of images all of the same size.\n",
    "        nrow (int, optional): Number of images displayed in each row of the grid.\n",
    "            The final grid size is ``(B / nrow, nrow)``. Default: ``8``.\n",
    "        padding (int, optional): amount of padding. Default: ``2``.\n",
    "        normalize (bool, optional): If True, shift the image to the range (0, 1),\n",
    "            by the min and max values specified by ``value_range``. Default: ``False``.\n",
    "        value_range (tuple, optional): tuple (min, max) where min and max are numbers,\n",
    "            then these numbers are used to normalize the image. By default, min and max\n",
    "            are computed from the tensor.\n",
    "        scale_each (bool, optional): If ``True``, scale each image in the batch of\n",
    "            images separately rather than the (min, max) over all images. Default: ``False``.\n",
    "        pad_value (float, optional): Value for the padded pixels. Default: ``0``.\n",
    "    Returns:\n",
    "        grid (Tensor): the tensor containing grid of images.\n",
    "    \"\"\"\n",
    "    if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "        _log_api_usage_once(make_grid)\n",
    "    if not torch.is_tensor(tensor):\n",
    "        if isinstance(tensor, list):\n",
    "            for t in tensor:\n",
    "                if not torch.is_tensor(t):\n",
    "                    raise TypeError(f\"tensor or list of tensors expected, got a list containing {type(t)}\")\n",
    "        else:\n",
    "            raise TypeError(f\"tensor or list of tensors expected, got {type(tensor)}\")\n",
    "\n",
    "    # if list of tensors, convert to a 4D mini-batch Tensor\n",
    "    if isinstance(tensor, list):\n",
    "        tensor = torch.stack(tensor, dim=0)\n",
    "\n",
    "    if tensor.dim() == 2:  # single image H x W\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "    if tensor.dim() == 3:  # single image\n",
    "        if tensor.size(0) == 1:  # if single-channel, convert to 3-channel\n",
    "            tensor = torch.cat((tensor, tensor, tensor), 0)\n",
    "        tensor = tensor.unsqueeze(0)\n",
    "\n",
    "    if tensor.dim() == 4 and tensor.size(1) == 1:  # single-channel images\n",
    "        tensor = torch.cat((tensor, tensor, tensor), 1)\n",
    "\n",
    "    if normalize is True:\n",
    "        tensor = tensor.clone()  # avoid modifying tensor in-place\n",
    "        if value_range is not None and not isinstance(value_range, tuple):\n",
    "            raise TypeError(\"value_range has to be a tuple (min, max) if specified. min and max are numbers\")\n",
    "\n",
    "        def norm_ip(img, low, high):\n",
    "            img.clamp_(min=low, max=high)\n",
    "            img.sub_(low).div_(max(high - low, 1e-5))\n",
    "\n",
    "        def norm_range(t, value_range):\n",
    "            if value_range is not None:\n",
    "                norm_ip(t, value_range[0], value_range[1])\n",
    "            else:\n",
    "                norm_ip(t, float(t.min()), float(t.max()))\n",
    "\n",
    "        if scale_each is True:\n",
    "            for t in tensor:  # loop over mini-batch dimension\n",
    "                norm_range(t, value_range)\n",
    "        else:\n",
    "            norm_range(tensor, value_range)\n",
    "\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        raise TypeError(\"tensor should be of type torch.Tensor\")\n",
    "    if tensor.size(0) == 1:\n",
    "        return tensor.squeeze(0)\n",
    "\n",
    "    # make the mini-batch of images into a grid\n",
    "    nmaps = tensor.size(0)\n",
    "    xmaps = min(nrow, nmaps)\n",
    "    ymaps = int(math.ceil(float(nmaps) / xmaps))\n",
    "    height, width = int(tensor.size(2) + padding), int(tensor.size(3) + padding)\n",
    "    num_channels = tensor.size(1)\n",
    "    grid = tensor.new_full((num_channels, height * ymaps + padding, width * xmaps + padding), pad_value)\n",
    "    k = 0\n",
    "    for y in range(ymaps):\n",
    "        for x in range(xmaps):\n",
    "            if k >= nmaps:\n",
    "                break\n",
    "            # Tensor.copy_() is a valid method but seems to be missing from the stubs\n",
    "            # https://pytorch.org/docs/stable/tensors.html#torch.Tensor.copy_\n",
    "            grid.narrow(1, y * height + padding, height - padding).narrow(  # type: ignore[attr-defined]\n",
    "                2, x * width + padding, width - padding\n",
    "            ).copy_(tensor[k])\n",
    "            k = k + 1\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fa81aeda-c167-4487-9179-cb2d9d441f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n01440764_2708\n",
      "/export/hdd/scratch/dataset/imagenet/train/n01440764/n01440764_2708.JPEG\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_files, train_imageNames = load_image_path(train_set)\n",
    "train_image_num = len(train_files)\n",
    "train_image_num = 10\n",
    "\n",
    "image_w = 224\n",
    "image_h = 224\n",
    "\n",
    "image = np.zeros([train_image_num,3,image_w,image_h])\n",
    "#image_test = np.zeros([10000,3,32,32])\n",
    "idx_offset = 0\n",
    "#time_start = time.time()\n",
    "for i in range (train_image_num):\n",
    "    \n",
    "    img = cv2.imread(train_files[idx_offset + i])\n",
    "    \n",
    "    dim = (image_w, image_h)\n",
    "    img = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    image[i] = img\n",
    "#time_end = time.time()\n",
    "#time_total  = time_end - time_start\n",
    "#print(\"the total decoding time is:%.6f\"%(time_total))\n",
    "#print(image[0][2][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ebe8281-2724-48b7-9145-6af561f66858",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_appendix = f'_{num_layers}x{layer_size}_{w0:.0f}fq_{num_iters}ep_{train_image_num}im'\n",
    "logfile = open(logdir_logfiles + '/logfile' + dir_appendix + '.txt','w')\n",
    "results = {'fp_bpp': [], 'hp_bpp': [], 'fp_psnr': [], 'hp_psnr': []}\n",
    "\n",
    "train_times = []\n",
    "avg_psnr = 0\n",
    "total_train_time = 0\n",
    "avg_train_time = 0\n",
    "model_size = 0\n",
    "logfile.write('image_num total_inr_fitting_time best_training_psnr\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3c95ae82-09da-4c97-8397-f1ee45f9ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sine(nn.Module):\n",
    "    \"\"\"Sine activation with scaling.\n",
    "    Args:\n",
    "        w0 (float): Omega_0 parameter from SIREN paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, w0=1.):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class SirenLayer(nn.Module):\n",
    "    \"\"\"Implements a single SIREN layer.\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_out (int): Dimension of output.\n",
    "        w0 (float):\n",
    "        c (float): c value from SIREN paper used for weight initialization.\n",
    "        is_first (bool): Whether this is first layer of model.\n",
    "        use_bias (bool):\n",
    "        activation (torch.nn.Module): Activation function. If None, defaults to\n",
    "            Sine activation.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, w0=30., c=6., is_first=False,\n",
    "                 use_bias=True, activation=None):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.is_first = is_first\n",
    "\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=use_bias)\n",
    "\n",
    "        # Initialize layers following SIREN paper\n",
    "        w_std = (1 / dim_in) if self.is_first else (sqrt(c / dim_in) / w0)\n",
    "        nn.init.uniform_(self.linear.weight, -w_std, w_std)\n",
    "        if use_bias:\n",
    "            nn.init.uniform_(self.linear.bias, -w_std, w_std)\n",
    "\n",
    "        self.activation = Sine(w0) if activation is None else activation\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Siren(nn.Module):\n",
    "    \"\"\"SIREN model.\n",
    "    Args:\n",
    "        dim_in (int): Dimension of input.\n",
    "        dim_hidden (int): Dimension of hidden layers.\n",
    "        dim_out (int): Dimension of output.\n",
    "        num_layers (int): Number of layers.\n",
    "        w0 (float): Omega 0 from SIREN paper.\n",
    "        w0_initial (float): Omega 0 for first layer.\n",
    "        use_bias (bool):\n",
    "        final_activation (torch.nn.Module): Activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_hidden, dim_out, num_layers, w0=30.,\n",
    "                 w0_initial=30., use_bias=True, final_activation=None):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for ind in range(num_layers):\n",
    "            is_first = ind == 0\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = dim_in if is_first else dim_hidden\n",
    "\n",
    "            layers.append(SirenLayer(\n",
    "                dim_in=layer_dim_in,\n",
    "                dim_out=dim_hidden,\n",
    "                w0=layer_w0,\n",
    "                use_bias=use_bias,\n",
    "                is_first=is_first\n",
    "            ))\n",
    "\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "        final_activation = nn.Identity() if final_activation is None else final_activation\n",
    "        self.last_layer = SirenLayer(dim_in=dim_hidden, dim_out=dim_out, w0=w0,\n",
    "                                use_bias=use_bias, activation=final_activation)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        return self.last_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f18364a1-a4ee-4bf8-8328-30465d1e23ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "    def __init__(self, representation, lr=1e-3, print_freq=1):\n",
    "        \"\"\"Model to learn a representation of a single datapoint.\n",
    "        Args:\n",
    "            representation (siren.Siren): Neural net representation of image to\n",
    "                be trained.\n",
    "            lr (float): Learning rate to be used in Adam optimizer.\n",
    "            print_freq (int): Frequency with which to print losses.\n",
    "        \"\"\"\n",
    "        self.representation = representation\n",
    "        self.optimizer = torch.optim.Adam(self.representation.parameters(), lr=lr)\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=num_iters)\n",
    "        self.print_freq = print_freq\n",
    "        self.steps = 0  # Number of steps taken in training\n",
    "        self.loss_func = torch.nn.MSELoss()\n",
    "        self.best_vals = {'psnr': 0.0, 'loss': 1e8}\n",
    "        self.logs = {'psnr': [], 'loss': []}\n",
    "        # Store parameters of best model (in terms of highest PSNR achieved)\n",
    "        self.best_model = OrderedDict((k, v.detach().clone()) for k, v in self.representation.state_dict().items())\n",
    "\n",
    "    def train(self, coordinates, features, num_iters):\n",
    "        \"\"\"Fit neural net to image.\n",
    "        Args:\n",
    "            coordinates (torch.Tensor): Tensor of coordinates.\n",
    "                Shape (num_points, coordinate_dim).\n",
    "            features (torch.Tensor): Tensor of features. Shape (num_points, feature_dim).\n",
    "            num_iters (int): Number of iterations to train for.\n",
    "        \"\"\"\n",
    "        with tqdm.trange(num_iters, ncols=100) as t:\n",
    "            for i in t:\n",
    "                # Update model\n",
    "                self.optimizer.zero_grad()\n",
    "                predicted = self.representation(coordinates)\n",
    "                loss = self.loss_func(predicted, features)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Calculate psnr\n",
    "                psnr = get_clamped_psnr(predicted, features)\n",
    "\n",
    "                # Print results and update logs\n",
    "                log_dict = {'loss': loss.item(),\n",
    "                            'psnr': psnr,\n",
    "                            'best_psnr': self.best_vals['psnr']}\n",
    "                t.set_postfix(**log_dict)\n",
    "                for key in ['loss', 'psnr']:\n",
    "                    self.logs[key].append(log_dict[key])\n",
    "\n",
    "                # Update best values\n",
    "                if loss.item() < self.best_vals['loss']:\n",
    "                    self.best_vals['loss'] = loss.item()\n",
    "                if psnr > self.best_vals['psnr']:\n",
    "                    self.best_vals['psnr'] = psnr\n",
    "                    # If model achieves best PSNR seen during training, update\n",
    "                    # model\n",
    "                    if i > int(num_iters / 2.):\n",
    "                        for k, v in self.representation.state_dict().items():\n",
    "                            self.best_model[k].copy_(v)\n",
    "                self.scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e23727-53a6-4fc5-922a-b8e7a0e7c5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 10000/10000 [00:51<00:00, 192.79it/s, best_psnr=25.5, loss=0.00282, psnr=25.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 10000/10000 [00:52<00:00, 189.62it/s, best_psnr=27.2, loss=0.00193, psnr=27.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 10000/10000 [00:52<00:00, 191.22it/s, best_psnr=29.2, loss=0.00121, psnr=29.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 10000/10000 [00:52<00:00, 190.19it/s, best_psnr=29.3, loss=0.00117, psnr=29.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 10000/10000 [00:52<00:00, 190.34it/s, best_psnr=34.9, loss=0.000323, psnr=34.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████| 10000/10000 [00:52<00:00, 189.80it/s, best_psnr=27, loss=0.002, psnr=27]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 10000/10000 [00:52<00:00, 190.91it/s, best_psnr=35.5, loss=0.000286, psnr=35.5]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████| 10000/10000 [00:52<00:00, 191.20it/s, best_psnr=34.9, loss=0.000321, psnr=34.9]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████| 10000/10000 [00:51<00:00, 192.53it/s, best_psnr=25.3, loss=0.00297, psnr=25.3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the image is: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|████████▏      | 5487/10000 [00:28<00:27, 162.02it/s, best_psnr=38.7, loss=0.000143, psnr=38.4]"
     ]
    }
   ],
   "source": [
    "for i in range (train_image_num):\n",
    "    print(\"the image is:\",i)\n",
    "    logfile.write(f'{i} ')\n",
    "    \n",
    "    func_rep = Siren(\n",
    "            dim_in=2,\n",
    "            dim_hidden=layer_size,\n",
    "            dim_out=3,\n",
    "            num_layers=num_layers,\n",
    "            final_activation=torch.nn.Identity(),\n",
    "            w0_initial=w0_initial,\n",
    "            w0=w0\n",
    "        ).to(device)\n",
    "    dtype = torch.float32\n",
    "    img = torch.from_numpy(image[i]/255.0).to(device, dtype)\n",
    "    coordinates, features = to_coordinates_and_features(img)\n",
    "    coordinates, features = coordinates.to(device), features.to(device)\n",
    "    model_size = model_size_in_bits(func_rep) / 8192.\n",
    "    #print(f'Model size: {model_size:.1f}kB')\n",
    "    # Dictionary to register mean values (both full precision and half precision)\n",
    "    \n",
    "    #fp_bpp = bpp(model=func_rep, image=img)\n",
    "    #print(fp_bpp)\n",
    "    time_start_train = time.time()\n",
    "    trainer = Trainer(func_rep, lr=learning_rate)\n",
    "    trainer.train(coordinates, features, num_iters=num_iters)\n",
    "    time_stop_train = time.time()\n",
    "    time_train  = time_stop_train - time_start_train\n",
    "    train_times.append(time_train)\n",
    "    total_train_time += train_times[i]\n",
    "    \n",
    "    logfile.write(f'{time_train:.18f} ')\n",
    "    \n",
    "    logfile.write(f'{trainer.best_vals[\"psnr\"]:.2f}\\n')\n",
    "    \n",
    "    if i == train_image_num - 1:\n",
    "        #print(func_rep)\n",
    "        logfile.write(f'\\n{func_rep}\\n\\n')\n",
    "        fp_bpp = bpp(model=func_rep, image=img)\n",
    "        results['fp_bpp'].append(fp_bpp)\n",
    "        \n",
    "    results['fp_psnr'].append(trainer.best_vals['psnr'])\n",
    "    avg_psnr += results['fp_psnr'][i]\n",
    "    torch.save(trainer.best_model, logdir + f'/best_model_{i}.pt')\n",
    "    func_rep.load_state_dict(trainer.best_model)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img_recon = func_rep(coordinates).reshape(img.shape[1], img.shape[2], 3).permute(2, 0, 1)\n",
    "        #save_image(torch.clamp(img_recon, 0, 1).to('cpu'), log_dir_image + f'/fp_reconstruction_{i}.png')\n",
    "        img_array = img_recon.cpu().numpy()\n",
    "        np.save(logdir_array + f'/fp_reconstruction_{i}.npy', img_array)\n",
    "        \n",
    "        if not os.path.exists(log_dir_image + dir_appendix):\n",
    "            os.makedirs(log_dir_image + dir_appendix)\n",
    "        save_image_new(img_recon, log_dir_image + dir_appendix + f'/fp_reconstruction_{i}.png')\n",
    "    #print(f'Best training psnr: {trainer.best_vals[\"psnr\"]:.2f}')\n",
    "    \n",
    "avg_psnr = avg_psnr / train_image_num\n",
    "avg_train_time = total_train_time / train_image_num\n",
    "logfile.write(f'Summary:\\n')\n",
    "logfile.write(f'Num layers: {num_layers}\\n')\n",
    "logfile.write(f'Layer size: {layer_size}\\n')\n",
    "logfile.write(f'Number of Epochs: {num_iters}\\n')\n",
    "logfile.write(f'Size of dataset: {train_image_num}\\n\\n')\n",
    "logfile.write(f'Model size: {model_size:.1f}kB\\n')\n",
    "bpp_out = results['fp_bpp'][0]\n",
    "logfile.write(f'Bits per pixel(fp_bpp): {bpp_out}\\n')\n",
    "logfile.write(f'Average psnr: {avg_psnr}\\n')\n",
    "logfile.write(f'Total training time: {total_train_time}s\\n')\n",
    "logfile.write(f'Average training time: {avg_train_time}s\\n')\n",
    "logfile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "67e254c4-a03b-4abb-8dfa-37e6328327ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.5kB\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5e00cd3-ed66-4437-93c6-c90410dee9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "19d3c3c3-0086-45a6-bd55-8a45df2d8644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0784438775510204\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "98b19f7a-75d0-4fc9-9e7c-a8fd97f33a83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c0dc5511-3992-4bf7-b44b-5c0809288bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████| 10000/10000 [00:21<00:00, 458.89it/s, best_psnr=19.9, loss=0.0102, psnr=19.9]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best training psnr: 19.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "id": "2f51e4b4-4999-4c29-8ae6-3ad3fbf067a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results['fp_bpp'].append(fp_bpp)\n",
    "results['fp_psnr'].append(trainer.best_vals['psnr'])\n",
    "torch.save(trainer.best_model, logdir + f'/best_model_{i}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "id": "f5bc2455-e334-4864-9b60-319e9c452a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49999\n"
     ]
    }
   ],
   "source": [
    "print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "id": "ee575b46-f5e0-4a4a-b80a-b0e5c02a9d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "func_rep.load_state_dict(trainer.best_model)\n",
    "with torch.no_grad():\n",
    "        img_recon = func_rep(coordinates).reshape(img.shape[1], img.shape[2], 3).permute(2, 0, 1)\n",
    "        save_image(torch.clamp(img_recon, 0, 1).to('cpu'), log_dir_image + f'/fp_reconstruction_{i}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "6b953e67-7dbb-4751-a8ac-df3733db982b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(img_recon.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "id": "30b9b6f1-8593-43e1-926c-6f98aa584162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1389,  0.0635, -0.0036,  ...,  0.2952,  0.3282,  0.3463],\n",
      "         [-0.0125, -0.0767, -0.0783,  ...,  0.2622,  0.2763,  0.2973],\n",
      "         [-0.0687, -0.0943, -0.0322,  ...,  0.2403,  0.2442,  0.2614],\n",
      "         ...,\n",
      "         [ 0.3581,  0.2421,  0.2126,  ...,  0.2389,  0.1364, -0.0173],\n",
      "         [ 0.3538,  0.2395,  0.1990,  ...,  0.2623,  0.2198,  0.1521],\n",
      "         [ 0.4097,  0.3302,  0.3110,  ...,  0.3763,  0.2916,  0.2280]],\n",
      "\n",
      "        [[ 0.2615,  0.1856,  0.1262,  ...,  0.4461,  0.4536,  0.4560],\n",
      "         [ 0.1033,  0.0370,  0.0490,  ...,  0.4086,  0.3998,  0.4060],\n",
      "         [ 0.0476,  0.0197,  0.0970,  ...,  0.3820,  0.3650,  0.3700],\n",
      "         ...,\n",
      "         [ 0.5831,  0.4676,  0.4376,  ...,  0.3810,  0.2638,  0.1026],\n",
      "         [ 0.5719,  0.4596,  0.4224,  ...,  0.4107,  0.3398,  0.2572],\n",
      "         [ 0.6219,  0.5522,  0.5404,  ...,  0.5558,  0.4363,  0.3429]],\n",
      "\n",
      "        [[ 0.4051,  0.3287,  0.2671,  ...,  0.5733,  0.5810,  0.5852],\n",
      "         [ 0.2440,  0.1738,  0.1811,  ...,  0.5462,  0.5404,  0.5474],\n",
      "         [ 0.1798,  0.1481,  0.2218,  ...,  0.5251,  0.5106,  0.5144],\n",
      "         ...,\n",
      "         [ 0.7244,  0.6217,  0.5956,  ...,  0.4950,  0.3796,  0.2283],\n",
      "         [ 0.7131,  0.6134,  0.5807,  ...,  0.5243,  0.4490,  0.3713],\n",
      "         [ 0.7570,  0.6963,  0.6867,  ...,  0.6646,  0.5488,  0.4624]]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(img_recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "bcf295cc-e88e-46f6-86e4-cbe2694408db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 0.5kB\n"
     ]
    }
   ],
   "source": [
    "print(f'Model size: {model_size:.1f}kB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "389d7ed4-66de-408b-92ee-7a01645efb14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Siren(\n",
      "  (net): Sequential(\n",
      "    (0): SirenLayer(\n",
      "      (linear): Linear(in_features=2, out_features=8, bias=True)\n",
      "      (activation): Sine()\n",
      "    )\n",
      "    (1): SirenLayer(\n",
      "      (linear): Linear(in_features=8, out_features=8, bias=True)\n",
      "      (activation): Sine()\n",
      "    )\n",
      "  )\n",
      "  (last_layer): SirenLayer(\n",
      "    (linear): Linear(in_features=8, out_features=3, bias=True)\n",
      "    (activation): Identity()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(func_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "4600abf6-8135-4d23-92df-64abd3bb3b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.84375\n"
     ]
    }
   ],
   "source": [
    "print(fp_bpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "e2505101-ae20-40d2-b775-aa5045ef0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76.0\n"
     ]
    }
   ],
   "source": [
    "print(image[0][1][4][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "34fc772d-8032-4d05-a609-440b91b11faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_t1 = img_recon * 255\n",
    "image_t1 = image_t1.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "2b3facbd-8e00-44f2-a352-d06bb935cb39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(image_t1[1][4][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "9596c3d9-0eb7-4f65-8fb6-33654c116f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_t1 = torch.clamp(image_t1,0,255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "f4756a95-edfd-4c7a-8eca-1eb2a5f4ed56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(90., device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(image_t1[1][4][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "d2b0585f-7f98-4528-8284-7a823a40d58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 32, 32])\n"
     ]
    }
   ],
   "source": [
    "print(image_t1.shape)\n",
    "image_array = image_t1.cpu().numpy()\n",
    "image_array = image_array.transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "f5662f8a-efc4-457e-8d34-59e3c42211d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv2.imwrite(\"1.jpg\", image_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166d8be6-50b2-4da6-9c13-8c772ddfbdef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903dbd4d-9f32-4cd5-b230-fc18d15fe61e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
